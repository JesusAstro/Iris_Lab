import csv
import cmath
import math
import numpy as np


class Neuron:
    # -----------------------Class initialization----------
    def __init__(self, csv_file):
        self.sepal_length = []
        self.sepal_width = []
        self.petal_length = []
        self.petal_width = []
        #self.plant_species = [][]
        rows, cols = (150, 3)
        #self.plant_species = [[0]*cols]*rows
        self.plant_species = [[0] * cols for _ in range(rows)]
        self.predicted_labels = []
        self.MAD = 0
        #self.weights = [1,-1,0.5,0]
        # Replace with hidden layer weights and output layers weights
        # initial weight were generated using the range of 
        #
        #   formula --> range| -2.4, 2.4 |
        #                    |  ---  --- |, where Fi is the number of inputs for neuron i
        #                    |   Fi   Fi |
        # formula derived from Multilayer Network slides
        # actual values were generated by in python using >>> round(random.uniform(-0.6,0.6),2)
        self.weights = [0.31, 0.38, -0.34]
        self.hidden_layer_weights = [ [0.31, 0.38, -0.34, -0.52],
                                     [-0.57,0.47,-0.01,-0.06],
                                     [0.26,-0.18,0.07,-0.53],
                                     ]
        self.output_layer_weights = [ [-0.59, 0.43, -0.21], 
                                     [-0.17,0.39,-0.16], 
                                     [0.3,-0.49,-0.34]
                                     ]
        self.biases = [0.38, -0.37, -0.29]
        self.biases2 = [0.36, 0.58, 0.17]
        self.bias = 1
        self.alpha = 0.05
        self.flag = 0 # flag to stop after no more changes are made
        self.changes = True
        self.count = 0;
        self.correct = 0
        with open(csv_file) as csvDataFile:
            csvReader = csv.reader(csvDataFile)
            for row in csvReader:
                self.sepal_length.append(float(row[0]))
                self.sepal_width.append(float(row[1]))
                self.petal_length.append(float(row[2]))
                self.petal_width.append(float(row[3]))
                self.plant_species[self.count][0] = float(row[4])
                self.plant_species[self.count][1] = float(row[5])
                self.plant_species[self.count][2] = float(row[6])
                self.count += 1
                #print(self.count)
    # -----------------------Methods-----------------------
    def printPredictions(self):
        if len(self.predicted_labels) == 0:
            print("Array is empty")
        else:
            for i in (self.predicted_labels):
                print(i);
    
    def adjustWeights(self, hOutputs, outputsL, err, index):
        gradientOutputLayer = []
        gradientHiddenLayer = []
        delta = []
        # backwards propagation
        # Caclulater gradient for output layer
        for i in range(3):
            gradientOutputLayer.append( outputsL[i] * (1.0 - outputsL[i]) * err[i] )
        #print("glo:" + str(gradientOutputLayer) )
        #
        
        # adjust hidden layer weights and bias
        for i in range(3):
            delta.append( gradientOutputLayer[0] * self.output_layer_weights[i][0] + gradientOutputLayer[1] * self.output_layer_weights[i][1] +
                    gradientOutputLayer[2] * self.output_layer_weights[i][2] ) 
            gradientHiddenLayer.append( hOutputs[i] * (1.0 - hOutputs[i]) * delta[i] )
        #print("del:" + str(delta) )
        #print("ghl:" + str(gradientHiddenLayer) )
        #print("hOut:" + str(hOutputs) )
        # adjust output layer weights and bias
        for j in range(3):
            for k in range(3):
                #print( str(self.output_layer_weights[j][k]) +" " + str(self.alpha) + " " + str(gradientOutputLayer[j]) + " " +str(hOutputs[k]) )
                self.output_layer_weights[j][k] = self.output_layer_weights[j][k] + self.alpha * gradientOutputLayer[j] * hOutputs[k]
            #st = self.alpha * gradientOutputLayer[j] * (-1.0)
            #print(str(st))
            self.biases2[j] = self.biases2[j] + self.alpha * gradientOutputLayer[j] * (-1.0)

        # 
        inputs = [self.sepal_length[index], self.sepal_width[index], self.petal_length[index], self.petal_width[index]]
        for i in range(3):
            for j in range(4):    
                self.hidden_layer_weights[i][j] = self.hidden_layer_weights[i][j] + self.alpha * gradientHiddenLayer[i] * inputs[j]
            self.biases[i] = self.biases[i] + self.alpha * gradientHiddenLayer[i] * (-1.0)
    def hLayer(self,index):
        # STEP 2 in NN for Hidden Layer
        Yi = []
        for k in range(3):
            Yi.append( (self.sepal_length[index]*self.hidden_layer_weights[k][0] + self.sepal_width[index]*self.hidden_layer_weights[k][1] + 
                self.petal_length[index]*self.hidden_layer_weights[k][2] + self.petal_width[index]*self.hidden_layer_weights[k][3]) - self.biases[k] )
        Yi[0] = 1.0/(1.0 + np.exp(-Yi[0]) )
        Yi[1] = 1.0/(1.0 + np.exp(-Yi[1]) )
        Yi[2] = 1.0/(1.0 + np.exp(-Yi[2]) )
        return Yi
    '''
    def activationFunc(self, hOutput):
        actual = []
        for k in range(3):
            actual.append( (hOutput[0] * self.output_layer_weights[k][0] + hOutput[1] * self.output_layer_weights[k][1] + 
                hOutput[2] * self.output_layer_weights[k][2]) - self.biases2[k] )
       
        actual[0] = 1.0/(1.0 + cmath.exp(-actual[0]) ) 
        actual[1] = 1.0/(1.0 + cmath.exp(-actual[1]) )
        actual[2] = 1.0/(1.0 + cmath.exp(-actual[2]) ) 
        return actual
    ''' 
    
    def activationFunc(self, hOutput):
        # STEP 2 in NN Activation for Output Layer
        actual = []
        for k in range(3):
            # Calculate the dot product for weights and inputs from hidden layer and subtract bias
            z = (hOutput[0] * self.output_layer_weights[k][0] + hOutput[1] * self.output_layer_weights[k][1] + 
                 hOutput[2] * self.output_layer_weights[k][2]) - self.biases2[k]
            #print("z: " + str(z))
            # Apply sigmoid activation function
            a = 1.0 / (1.0 + np.exp(-z))
            actual.append(a)
        return actual
    
    def errorFunc(self,index, yOutput):
        err = [self.plant_species[index][0] - yOutput[0] ,  self.plant_species[index][1] - yOutput[1], self.plant_species[index][2] - yOutput[2] ]
        return err
    def perceptronTraining(self):
        # ---------Initialization-------
        while self.flag < 100:
            #self.changes = False
            self.correct = 0
            self.MAD = 0
        # ---------Activation----------- 
            for i in range(0, len(self.sepal_length) ):          
                #outputStr = ""
                #outputStr += ("Iteration: " + str(i+1) + " weights are " + self.printWeights())
                # Activation Function call
                activation = self.hLayer(i)
                #print("act:" + str(activation) )
                prediction = self.activationFunc(activation)
                #print("pred:" + str(prediction) )
                error = self.errorFunc(i, prediction)
                #print(error)
                self.MAD += (abs(error[0]) + abs(error[1]) + abs(error[2]))
                # Back propagation
                self.adjustWeights(activation, prediction, error, i)
                #print("b1:" + str(self.biases) )
                #print("b2:" + str(self.biases2) )
                #print(error) 
                #exit()
                print("Epoch " + str(self.flag+1) + " Iteration " + str(i) + ": Prediction is " + str(prediction) )  
                
                if (i == 149):
                    print("Epoch " + str(self.flag+1) + " Results: MAD = " + str(self.MAD/150) )
            self.flag += 1
            
            print()
        '''# PRINT RESULT PERCENT
        res = self.correct / self.count
        print("Num of samples: " + " " + str(self.count) + " \nNum of correct predictions: " + str(self.correct))
        print(str(res) + " % correct!")'''

# Example usage:
neuron = Neuron('iris.csv')
neuron.perceptronTraining()

